[
{
	"uri": "//localhost:38235/1-introduce-aws/1.1-computeservices/",
	"title": "Compute Services",
	"tags": [],
	"description": "",
	"content": "Introduction AWS Compute Services Compute services offered by AWS provide scalable computing power for various workloads, enabling businesses to deploy and manage applications efficiently in the cloud. Here\u0026rsquo;s an introduction to some of the key compute services provided by AWS:\nContent 1. Amazon EC2 (Elastic Compute Cloud): Amazon EC2 is a web service that provides resizable compute capacity in the cloud. It allows users to quickly scale up or down based on demand, paying only for the resources they use. Users can choose from a variety of instance types optimized for different workloads, such as general-purpose, compute-optimized, memory-optimized, and storage-optimized instances. EC2 instances can be used for a wide range of applications including web hosting, application development, data processing, and machine learning.\nEC2 General-Purpose Instance Types Here are several general-purpose examples from which we can pick:\nT2. micro: The most well-known instance in AWS is t2. micro, which gives 1 CPU and 1 GB of memory with low to moderate network performance. It is also free and highly helpful for individuals first starting AWS.\rM6a Instance: The third-generation AMD EPYC processors used in the M6 instance are perfect for general-purpose tasks. In m6a there are different sizes like m6a.large, m6a.2xlarge, m6a.4xlarge, and so on. m6a large offers 2 CPUs, 8GiB memory, and network performance up to 12.5 Gigabit.\rM5 instance: The newest generation of general-purpose instances, known as M5, are powered by Intel’s Xeon Platinum 8175 processors. Its M5 divisions include m5. large, m5.12xlarge, and m5.24 large, and the sort of M5 service we select will depend on memory, CPUs, storage, and network speed.\rUse in which situation? Applications Web Servers: The web servers can be hosted in General-purpose instances.EC2 instances provide a flexible and scalable platform for web applications. Development and Test Environment: The developers can use these General-purpose instances to build, test and deploy the applications. It is a cost-effective solution for running this environment.\nContent delivery: The hosting of content delivery networks (CDNs) that distribute content to users all over the world is possible using general-purpose instances. EC2 instances can be set up to provide content with low latency and great performance. A popular option for many businesses, AWS EC2 general-purpose instances offer a versatile and scalable platform for a variety ###of applications.\n2. AWS Lambda: AWS Lambda is a serverless computing service provided by Amazon Web Services (AWS) that enables users to run code without provisioning or managing servers. It allows developers to focus on writing code for their applications without worrying about the underlying infrastructure. Here\u0026rsquo;s a detailed overview of AWS Lambda: Serverless Computing: With Lambda, users can upload their code and AWS takes care of provisioning, scaling, and managing the infrastructure needed to run that code. Users are charged only for the compute time consumed by their code, measured in milliseconds, making Lambda a cost-effective solution for various workloads.\nEvent-Driven Architecture: Lambda functions are invoked in response to events such as HTTP requests, changes to data in Amazon S3 buckets, updates to Amazon DynamoDB tables, messages from Amazon SNS (Simple Notification Service), and many others. This event-driven architecture allows developers to build reactive and scalable applications that respond to changes or triggers in real-time.\nSupported Runtimes and Languages: Lambda supports multiple programming languages including Node.js, Python, Java, Go, .NET Core, and Ruby, allowing developers to use their preferred language for writing Lambda functions. Each Lambda function runs in an isolated environment called a \u0026ldquo;runtime\u0026rdquo;, which provides the necessary dependencies and libraries for the chosen language.\nAutomatic Scaling: Lambda automatically scales the execution environment to handle incoming requests or events. It can handle a few requests per day to thousands of requests per second without any manual intervention. Scaling is handled by AWS based on the number of incoming events and the configured concurrency settings.\nStateless Execution: Lambda functions are stateless, meaning they do not maintain any persistent state between invocations. Each invocation of a Lambda function is independent of previous invocations.If state persistence is required, developers can use external storage services like Amazon DynamoDB or Amazon S3 to store and retrieve state information.\nIntegration with AWS Services: Lambda integrates seamlessly with other AWS services, allowing developers to build complex workflows and applications. For example, Lambda functions can be used to process data from Amazon S3, trigger notifications via Amazon SNS, or interact with Amazon RDS databases.\nAWS provides built-in integration with various services through AWS SDKs and event sources, making it easy to connect Lambda functions with other AWS resources. Security and Access Control:\nLambda functions can be secured using AWS IAM (Identity and Access Management) policies, which control who can invoke the function and what resources it can access. Developers can also use AWS Key Management Service (KMS) to encrypt sensitive data within Lambda functions or use environment variables to store configuration settings securely.\nUse lamdba in which situation? Microservices Architecture:\nLambda functions can be used to implement microservices in a serverless architecture, where each function represents a specific component or functionality of the application. By breaking down your application into smaller, independent functions, you can achieve better scalability, flexibility, and maintainability, while reducing the operational overhead of managing infrastructure.\nReal-time Data Processing:\nLambda is well-suited for real-time data processing tasks such as stream processing, data enrichment, and analytics. It can be used to process streaming data from services like Amazon Kinesis and Amazon Managed Streaming for Apache Kafka (Amazon MSK). By processing data in real-time with Lambda, you can derive insights, trigger actions, and make decisions based on up-to-date information, enabling reactive and data-driven applications.\nScheduled Tasks and Cron Jobs:\nLambda functions can be scheduled to run at specified intervals using CloudWatch Events or EventBridge (formerly CloudWatch Events). This makes Lambda a convenient solution for running scheduled tasks, cron jobs, and batch processing jobs without the need for dedicated servers or infrastructure. You can use Lambda to perform periodic data backups, generate reports, clean up resources, or execute any other recurring tasks.\nWeb Application Backends:\nLambda can serve as the backend for web applications, handling HTTP requests and executing application logic in response. When combined with API Gateway, Lambda enables developers to build serverless RESTful APIs quickly and easily. Lambda functions can handle user authentication, data validation, business logic, and database interactions, providing a scalable and cost-effective solution for web application development.\n3. Amazon ECS (Elastic Container Service): Amazon ECS is a fully managed container orchestration service that allows users to run, stop, and manage Docker containers on a cluster of EC2 instances. Users can easily deploy, manage, and scale containerized applications using ECS. It integrates with other AWS services like Elastic Load Balancing, IAM, and CloudWatch for enhanced functionality. ECS enables users to build microservices architectures and modernize their applications by leveraging containers.\n4. Amazon EKS (Elastic Kubernetes Service): Amazon EKS is a fully managed Kubernetes service that makes it easy to deploy, manage, and scale containerized applications using Kubernetes on AWS. It provides a highly available and secure Kubernetes control plane without the overhead of managing the infrastructure. With Amazon EKS, users can run Kubernetes applications with the same tools and APIs they use on-premises, while benefiting from the scalability and reliability of AWS.Real-time Data Processing:.\n"
},
{
	"uri": "//localhost:38235/1-introduce-aws/",
	"title": "Introduce AWS",
	"tags": [],
	"description": "",
	"content": "Overview AWS stands for Amazon Web Services. It is a comprehensive and widely-used cloud computing platform provided by Amazon.com. Launched in 2006, AWS offers a broad set of services including computing power, storage options, networking capabilities, databases, machine learning, and AI tools, among others, all delivered over the internet.\nBusinesses and individuals can leverage AWS to build and deploy applications and services with greater flexibility, scalability, and reliability compared to traditional on-premises infrastructure. AWS operates on a pay-as-you-go pricing model, allowing users to only pay for the resources they consume, which can help reduce costs and optimize spending.\nSome key features and benefits of AWS include:\nScalability: AWS allows users to scale resources up or down based on demand, ensuring optimal performance and cost efficiency.\nReliability: AWS offers a highly reliable infrastructure with data centers located in regions around the world, providing redundancy and ensuring high availability.\nSecurity: AWS employs robust security measures to protect data and infrastructure, including encryption, identity and access management, and compliance certifications.\nFlexibility: With a wide range of services and configurations available, AWS can accommodate diverse workloads and use cases, from simple web hosting to complex data analytics.\nGlobal Reach: AWS operates in multiple geographic regions, allowing users to deploy applications closer to their end-users for lower latency and improved performance.\nOverall: AWS has become a cornerstone of modern cloud computing, powering millions of businesses and organizations worldwide with its extensive suite of services and infrastructure solutions.\nContent Introduce about keys service we will use on labs hands-on:\nCompute Services Storage Services Database Services Networking Services "
},
{
	"uri": "//localhost:38235/",
	"title": "Introduce Series 100 Days Cloud",
	"tags": [],
	"description": "",
	"content": "Introduce Series 100 Days Cloud Overview AWS stands for Amazon Web Services. It is a comprehensive and widely-used cloud computing platform provided by Amazon.com. Launched in 2006, AWS offers a broad set of services including computing power, storage options, networking capabilities, databases, machine learning, and AI tools, among others, all delivered over the internet.\nContent "
},
{
	"uri": "//localhost:38235/2-hands-on/2.1-build-serverless-application/",
	"title": "Build Serverless Application",
	"tags": [],
	"description": "",
	"content": "Overview The Take Note App Project is a serverless application designed to provide users with a convenient platform for taking and managing notes. Built using AWS services, the project leverages Lambda functions for backend logic and DynamoDB for data storage. Additionally, it integrates with GitHub for automatic deployment using AWS CodeBuild.\nKey components and features of the Take Note App Project include:\nServerless Architecture: The project follows a serverless architecture, eliminating the need for managing servers and infrastructure. AWS Lambda functions are used to execute backend logic in response to events triggered by user actions. Lambda Functions: Lambda functions serve as the backbone of the application, handling various functionalities such as creating, retrieving, updating, and deleting notes. These functions are designed to scale automatically based on demand, ensuring high availability and optimal performance. DynamoDB Integration: DynamoDB is utilized as the NoSQL database for storing and managing notes data. The integration between Lambda functions and DynamoDB enables seamless interaction with the database, allowing users to store and retrieve their notes efficiently. GitHub Integration: The project utilizes AWS CodeBuild for continuous integration and deployment (CI/CD) by integrating with GitHub repositories. Whenever changes are pushed to the GitHub repository, CodeBuild automatically triggers the build process, ensuring that the latest version of the application is deployed to the AWS environment. User Authentication and Authorization: The application can incorporate user authentication and authorization mechanisms to ensure secure access to notes data. This can be achieved using AWS Cognito for user management and authentication, allowing users to sign in securely and access their notes with proper permissions. Frontend Interface: While not explicitly mentioned, the project likely includes a frontend interface, such as a web application or mobile app, through which users interact with the Take Note App. This interface communicates with the backend Lambda functions to perform CRUD operations on notes stored in DynamoDB. Overall, the Take Note App Project demonstrates the power and flexibility of serverless architecture in building scalable and cost-effective applications. By leveraging AWS services such as Lambda, DynamoDB, CodeBuild, and potentially others, the project provides a robust platform for users to manage their notes seamlessly while automating the deployment process for streamlined development workflows.\nContent "
},
{
	"uri": "//localhost:38235/2-hands-on/2.1-build-serverless-application/2.1.1.-create-vpc-subnet-route-table/",
	"title": "Create VPC, Subnet, Route Table",
	"tags": [],
	"description": "",
	"content": "Overview Let’s delve into the concepts of VPC, Subnet, Route Table, and Security Group:\n1.1 Virtual Private Cloud (VPC):\nA Virtual Private Cloud (VPC) is a virtual network environment provided by Amazon Web Services (AWS).\nIt allows you to provision a logically isolated section of the AWS cloud where you can launch AWS resources such as EC2 instances, RDS databases, and Lambda functions.\nWith a VPC, you have complete control over your network environment, including IP address ranges, subnets, route tables, and network gateways.\nVPCs provide security by enabling you to define network access control policies, set up VPN connections, and use security groups and network ACLs to restrict traffic.\n1.2 Subnet:\nA subnet is a segmented portion of a VPC’s IP address range in which you can place AWS resources.\nSubnets allow you to organize resources within a VPC and define separate network segments with their own routing configurations, access control policies, and availability zones.\nEach subnet is associated with a specific availability zone (AZ) within a region, and resources deployed in that subnet reside in the corresponding AZ.\nSubnets can be public or private, depending on whether they have a route to an internet gateway.\n1.3 Route Table:\nA route table is a set of rules (routes) that determine where network traffic is directed within a VPC.\nEach subnet in a VPC must be associated with a route table, which defines how traffic is routed in and out of the subnet.\nRoute tables contain routes to specific destinations, such as internet gateways, virtual private gateways (for VPN connections), VPC peering connections, or NAT gateways.\nBy configuring routes in the route table, you can control how traffic flows between subnets within the VPC and to external networks.\n1.4 Security Group:\nA security group acts as a virtual firewall for your instances to control inbound and outbound traffic.\nIt operates at the instance level and can be associated with multiple instances within a VPC.\nSecurity groups allow you to define rules that permit or deny traffic based on protocols, ports, and IP addresses.\nBy default, all inbound traffic is denied, and all outbound traffic is allowed, but you can customize these rules to meet your specific security requirements.\nSecurity groups are stateful, meaning that if you allow traffic for a specific protocol and port, the return traffic is automatically allowed regardless of the outbound rules.\nIn summary, VPCs provide isolated network environments within AWS, subnets allow you to segment your VPC into smaller networks, route tables determine how traffic is routed within the VPC, and security groups control the traffic flow to and from your instances. These components work together to provide a secure and scalable networking infrastructure in the AWS cloud.\nHands-on Let’s Create VPC follow instructions below: Go to https://us-east-1.console.aws.amazon.com/vpcconsole/home?region=us-east-1#vpcs choose Create VPC Select VPC and More Select IPv4 CIDR: 10.0.0.0/16 Choose AZ: 1 Choose number public subnet 1 Choose private subnet 1 Choose NAT gateway: none, because we are connecting within AWS resources, so we don’t need any connection to the internet. NAT gateway suitable for third party resources on the internet After fulfilling these options click to Create VPC. Waiting a minute to resources provisioning, After done, we have a vpc like this picture above. "
},
{
	"uri": "//localhost:38235/2-hands-on/",
	"title": "Hands-on Lab",
	"tags": [],
	"description": "",
	"content": "Overview AWS stands for Amazon Web Services. It is a comprehensive and widely-used cloud computing platform provided by Amazon.com. Launched in 2006, AWS offers a broad set of services including computing power, storage options, networking capabilities, databases, machine learning, and AI tools, among others, all delivered over the internet.\nContent "
},
{
	"uri": "//localhost:38235/1-introduce-aws/1.2-storageservices/",
	"title": "Storage Services",
	"tags": [],
	"description": "",
	"content": "Introduction AWS Storage Services AWS offers a comprehensive suite of storage services designed to meet the diverse needs of businesses and developers, ranging from simple object storage to high-performance block storage.\nContent 1. Amazon S3 (Simple Storage Service): Amazon S3 is a scalable object storage service designed to store and retrieve any amount of data from anywhere on the web. It is highly durable, secure, and cost-effective. S3 provides features such as versioning, encryption, access control, and lifecycle management to manage data effectively.\nIt is commonly used for a wide range of use cases, including backup and restore, data archiving, content distribution, and hosting static websites.\nObject Storage Classes: Amazon S3 Standard (S3 Standard): S3 Standard offers high durability, availability, and performance object storage for frequently accessed data. Because it delivers low latency and high throughput, S3 Standard is appropriate for a wide variety of use cases, including cloud applications, dynamic websites, content distribution, mobile and gaming applications, and big data analytics.\nKey features:\nGeneral purpose storage for frequently accessed data Low latency and high throughput performance Designed to deliver 99.99% availability with an availability SLA of 99.9% Amazon S3 Intelligent-Tiering (S3 Intelligent-Tiering): often abbreviated as S3 Intelligent-Tiering, is a storage class introduced by Amazon Web Services (AWS) for their Simple Storage Service (S3). It is designed to automatically optimize storage costs by moving data between two access tiers: frequent access and infrequent access, based on usage patterns.\nHow S3 Intelligent-Tiering works:\nAutomatic Tiering: S3 Intelligent-Tiering automatically monitors the access patterns of your data and moves objects between two access tiers: frequent access and infrequent access. Data that is frequently accessed remains in the frequent access tier, while data that hasn\u0026rsquo;t been accessed for a period of time is moved to the infrequent access tier.\nCost Optimization: By automatically moving data between access tiers, S3 Intelligent-Tiering helps optimize storage costs. You don\u0026rsquo;t need to manually manage the movement of data between tiers, reducing operational overhead.\nNo Retrieval Fees: Unlike some other storage classes in S3, S3 Intelligent-Tiering does not incur retrieval fees when accessing data from the infrequent access tier. This makes it particularly cost-effective for data with unpredictable access patterns.\nDurability and Availability: S3 Intelligent-Tiering provides the same high durability and availability as other S3 storage classes, ensuring that your data is protected against hardware failures and is accessible when needed.\nMonitoring and Analytics: AWS provides tools for monitoring and analyzing the access patterns of your data stored in S3 Intelligent-Tiering. This allows you to gain insights into your data usage and make informed decisions about storage optimization.\nUse S3 in which situation? Data Backup and Archiving: S3 provides a reliable and cost-effective solution for storing backups and archives of data. Its high durability ensures that data remains safe over extended periods, making it ideal for long-term storage requirements.\nStatic Website Hosting: S3 can host static websites by storing HTML, CSS, JavaScript, and other web assets. It integrates seamlessly with other AWS services like Amazon Route 53 for DNS routing and Amazon CloudFront for content delivery, enabling fast and scalable website hosting.\nContent Distribution: S3 combined with Amazon CloudFront allows you to distribute content globally with low latency and high transfer speeds. This is useful for delivering large files, streaming media, or website assets to users worldwide.\nApplication Data Storage: Many applications use S3 as a central storage repository for various types of data, such as user-generated content, media files, application logs, and configuration files. S3\u0026rsquo;s scalability and reliability make it suitable for storing large volumes of diverse data types.\nBig Data Analytics: S3 serves as a data lake for storing vast amounts of structured and unstructured data that can be analyzed using AWS analytics services like Amazon Athena, Amazon Redshift Spectrum, or AWS Glue. Data stored in S3 can be queried directly without the need for data movement.\nDisaster Recovery: S3 can be part of a disaster recovery strategy, where critical data backups are stored in S3 buckets across different AWS regions. In the event of a disaster, data can be quickly restored from S3 to maintain business continuity.\nData Warehousing: S3 can act as a staging area for data ingested into data warehouses like Amazon Redshift or Amazon Athena. Data is first loaded into S3 and then processed by the analytics services, allowing for scalable and cost-effective data storage.\nMobile and IoT Applications: S3 is often used as a storage backend for mobile and IoT applications to store user-generated content, sensor data, and application logs. Its scalability and compatibility with AWS SDKs make it easy to integrate with various application platforms.\n2. Amazon EBS (Elastic Block Store): Amazon Elastic Block Store (EBS) is a block-level storage service provided by Amazon Web Services (AWS) that is designed for use with Amazon EC2 (Elastic Compute Cloud) instances.\nBlock-Level Storage:\nAmazon EBS provides block-level storage volumes that are similar to physical hard drives or SSDs, allowing users to create and attach storage volumes to EC2 instances. These volumes appear as raw block devices to the EC2 instances and can be formatted and mounted like any other block storage device. Persistence and Durability:\nEBS volumes are designed for durability and reliability. Data stored on EBS volumes is replicated within the same Availability Zone to ensure data durability. EBS volumes are persistent, meaning that data persists even after the associated EC2 instance is terminated. Users can detach EBS volumes from one EC2 instance and attach them to another without losing data. High Performance:\nEBS volumes offer high-performance storage options, including SSD-backed volumes (EBS-SSD) and magnetic volumes (EBS-HDD), to meet the performance requirements of different workloads. SSD-backed volumes provide low-latency and high IOPS (Input/Output Operations Per Second) for performance-sensitive applications, while magnetic volumes offer cost-effective storage for less demanding workloads. Snapshot and Backup:\nEBS volumes support snapshots, which are point-in-time backups of the volume data stored in Amazon S3. Users can create snapshots of EBS volumes manually or automatically using scheduled snapshots. Snapshots are incremental, meaning that only the changed blocks since the last snapshot are stored, which helps reduce storage costs and backup times. Encryption and Security:\nEBS volumes support encryption using AWS Key Management Service (KMS), allowing users to encrypt data at rest to meet compliance and security requirements. Users can also specify encryption settings when creating new EBS volumes or encrypt existing volumes using snapshots. Scalability and Flexibility:\nEBS volumes can be resized dynamically without any downtime, allowing users to scale storage capacity based on changing workload requirements. Users can choose from a range of volume types and sizes to meet the performance and capacity needs of their applications. Integration with AWS Services\nEBS volumes can be integrated with other AWS services such as Amazon EC2, Amazon RDS (Relational Database Service), and AWS Lambda to provide storage for various types of applications and workloads. Use EBS in which situation? Amazon Elastic Block Store (EBS) is used in various situations where persistent and scalable block storage is required for applications running on single Amazon EC2 instance 3. Amazon EFS (Elastic File System) Amazon EFS is a scalable and fully managed file storage service designed to provide scalable and shared access to files from multiple EC2 instances. It supports NFS (Network File System) protocol and can be seamlessly integrated with existing applications and workflows.\nEFS is suitable for use cases such as content management, media processing, software development, and data analytics that require shared file storage.\nUse EFS in which situation? Amazon EFS is commonly used in situations where multiple EC2 instances need shared access to a common file system. It provides a centralized and scalable storage solution for applications that require shared file storage\n4. Amazon FSx (File Storage) Amazon FSx offers fully managed file storage services optimized for specific use cases, including Amazon FSx for Windows File Server and Amazon FSx for Lustre. FSx for Windows File Server provides fully managed, highly available Windows file shares, while FSx for Lustre delivers high-performance file systems for compute-intensive workloads.\nFSx is suitable for applications that require Windows-compatible file storage or high-performance file systems for compute-intensive workloads such as machine learning and simulation.\nUse FSx in which situation? Amazon FSx for Windows File Server is used in situations where users need a fully managed Windows-compatible file system with support for SMB (Server Message Block) protocol. It provides a scalable and fully managed Windows file system that is accessible from Windows, Linux, and macOS clients, making it suitable for file sharing and collaboration across different platforms. "
},
{
	"uri": "//localhost:38235/2-hands-on/2.1-build-serverless-application/2.1.2.-create-vpc-endpoint/",
	"title": "Create VPC endpoint",
	"tags": [],
	"description": "",
	"content": "Overview We see VPC Endpoint connect between Lambda function and DynamoDB.\nWhy need a VPC endpoint to connect it?\nVPC Isolation: When Lambda functions are configured to run within a VPC, they are isolated from the internet and can only access resources within the VPC or those reachable through VPC endpoints. This ensures a higher level of security by restricting external access. DynamoDB Endpoint: DynamoDB is an AWS service that resides on the AWS network infrastructure. When accessed from within a VPC, Lambda functions need to communicate with DynamoDB through an interface known as a VPC endpoint. A VPC endpoint acts as a gateway that enables private communication between resources in your VPC and AWS services like DynamoDB without traversing the internet. Private Subnet Access: Placing both Lambda functions and DynamoDB in the same private subnet ensures that communication between them remains private and does not traverse the public internet. This further enhances the security posture of the application. By creating a VPC endpoint for DynamoDB within your VPC, Lambda functions in the same VPC can securely communicate with DynamoDB without the need for internet access. This setup ensures that data transmission between Lambda and DynamoDB remains secure and private, while also optimizing network performance by avoiding unnecessary internet routing. In summary, while Lambda functions and DynamoDB can coexist in the same private subnet within a VPC, Lambda functions require a VPC endpoint to establish connectivity with DynamoDB when operating within a VPC environment. This ensures secure and private communication between the two services without internet access to doesn\u0026rsquo;t pay fee for internet outbound from AWS Pricing Model.\nHands-on Let’s Create VPC Endpoint follow instructions below: Go to https://us-east-1.console.aws.amazon.com/vpcconsole/home?region=us-east-1#Endpoints: choose Create Endpoint Choose Option below like picture:\nName tag – optional: Fill name of VPC endpoint Service category: AWS services Services: com.amazonaws.us-east-1.dynamodb VPC: select vpc we created at Step 2.1.1 "
},
{
	"uri": "//localhost:38235/1-introduce-aws/1.3-databaseservice/",
	"title": "Database Service",
	"tags": [],
	"description": "",
	"content": "Introduction Database Service Amazon Web Services (AWS) offers a comprehensive suite of cloud-based services, including its highly popular Database as a Service (DBaaS) offerings. AWS\u0026rsquo;s Database service provides users with scalable, reliable, and cost-effective solutions for managing various types of data, ranging from simple key-value pairs to complex relational databases. These services are designed to streamline database management tasks, enhance performance, and ensure high availability and security.\nAWS offers several database services, each catering to different use cases and workload requirements Contents Amazon RDS (Relational Database Service): Amazon Relational Database Service (Amazon RDS) is a managed database service provided by Amazon Web Services (AWS) that simplifies the process of setting up, operating, and scaling relational databases in the cloud. With Amazon RDS, users can deploy, manage, and scale popular relational database engines such as MySQL, PostgreSQL, MariaDB, Oracle, and SQL Server without the need for significant administrative overhead.\nKey feature:\nManaged Service: Amazon RDS is a fully managed service, which means AWS handles routine database administration tasks such as hardware provisioning, database setup, patching, backups, and monitoring. This allows users to focus on building applications rather than managing infrastructure.\nMultiple Database Engines: Amazon RDS supports several popular relational database engines, including MySQL, PostgreSQL, MariaDB, Oracle, and SQL Server. Users can choose the engine that best fits their application requirements and leverage familiar database technologies.\nScalability: Amazon RDS allows users to easily scale their database instances up or down based on changing workload demands. Users can increase or decrease compute and storage resources without downtime, enabling seamless scalability as application usage fluctuates.\nHigh Availability: Amazon RDS offers built-in high availability features such as automated backups, automated failover, and Multi-AZ deployments. Multi-AZ deployments replicate data across multiple availability zones to provide fault tolerance and ensure data durability and availability in the event of a hardware failure or outage.\nSecurity: Amazon RDS provides robust security features to protect data stored in databases. These include encryption at rest and in transit, network isolation using Amazon VPC, IAM authentication integration, and database instance access controls.\nPerformance Monitoring and Optimization: Amazon RDS offers performance monitoring tools and metrics through Amazon CloudWatch, enabling users to monitor database performance, set alarms, and troubleshoot performance issues. Users can also leverage features like Read Replicas and Amazon Aurora for improved performance and scalability.\nAutomated Maintenance: Amazon RDS automates routine database maintenance tasks such as software patching, hardware provisioning, and backups. This ensures that databases remain up-to-date with the latest security patches and optimizations without manual intervention.\nCost-Effective: Amazon RDS offers a pay-as-you-go pricing model, where users pay only for the resources they consume on an hourly basis. This helps reduce upfront costs and allows for cost-effective scaling based on actual usage patterns.\nAmazon Aurora: Aurora is a MySQL and PostgreSQL-compatible relational database engine built for the cloud. It offers high performance, scalability, and availability, with features like automated backups, continuous monitoring, and multi-region replication.\nKey Feature:\n- Performance: According to the official website, Aurora offers up to 5x the throughput of MySQL and 3x the throughput of PostgreSQL.\nThis benchmark suggesting Aurora can be 60 times faster than RDS.\nAurora provides better write performance because it reduces the write amplification by only sending the redo log to the remote storage service, which eliminates other writes during transaction commit path such as the infamous double-write buffer.\nAurora provides better read scalability because of the log-based architecture, it can support up to 15 read-replicas. RDS can only support 5, RDS doesn\u0026rsquo;t support more because the classic streaming replication carries more performance penalty on the primary. Aurora also incurs much lower replication lags, especially under write-heavy load.\n- Scalability: Aurora is highly scalable, allowing you to easily increase or decrease the size of your database instance as needed. It can automatically scale storage capacity up to 64 TB per database instance, and it supports up to 15 read replicas for read scalability.\n- High Availability: Aurora provides built-in high availability and fault tolerance. It replicates data across multiple Availability Zones (AZs) within a region, ensuring that your database remains available even in the event of AZ failures.\n- Durability: Aurora automatically backs up your database to Amazon S3 continuously, ensuring data durability and allowing point-in-time recovery to any second in the past for up to 35 days.\n- Compatibility: Aurora is compatible with MySQL and PostgreSQL, allowing you to use familiar database engines and tools while leveraging the benefits of Aurora\u0026rsquo;s performance and scalability.\n- Security: Aurora provides robust security features, including network isolation using Amazon VPC, encryption at rest and in transit using AWS Key Management Service (KMS), and fine-grained access control using IAM roles and database-level permissions.\n- Cost-Effectiveness: Aurora is designed to be cost-effective, with a pay-as-you-go pricing model based on your actual database usage. It offers significant cost savings compared to traditional on-premises databases, particularly for workloads with fluctuating demand.\nAmazon DynamoDB: Amazon DynamoDB is a fully managed NoSQL database service provided by Amazon Web Services (AWS). It is designed to provide seamless scalability, high performance, and low-latency data access for applications requiring single-digit millisecond response times. Some key features of Amazon DynamoDB include: Key Feature:\nFully Managed: DynamoDB is a fully managed service, which means AWS handles tasks such as hardware provisioning, setup, configuration, replication, software patching, and backups. This allows developers to focus on building applications without worrying about infrastructure management.\nScalability: DynamoDB is designed to scale effortlessly to accommodate varying workloads and data volumes. It automatically scales both read and write capacity based on your application\u0026rsquo;s traffic patterns, and it can handle trillions of requests per day across thousands of partitions.\nPerformance: DynamoDB offers consistent, single-digit millisecond response times for read and write operations, regardless of the size of your data or the level of traffic. This makes it suitable for low-latency applications that require rapid data access.\nFlexible Data Model: DynamoDB supports both key-value and document data models, allowing you to store and query structured, semi-structured, or unstructured data. It also provides support for nested data types, complex data structures, and document-based queries.\nSecurity and Compliance: DynamoDB offers robust security features, including encryption at rest and in transit, fine-grained access control using AWS Identity and Access Management (IAM) policies, and integration with AWS Key Management Service (KMS) for managing encryption keys. It is also compliant with various industry standards and regulations, such as HIPAA, GDPR, and PCI DSS.\nAutomatic Backup and Restore: DynamoDB automatically backs up your data and maintains incremental backups for up to 35 days, allowing you to restore your database to any point in time within that window. This helps protect against data loss and provides a mechanism for disaster recovery.\nGlobal Tables: DynamoDB Global Tables allow you to replicate your data across multiple AWS Regions worldwide, enabling low-latency data access for globally distributed applications. It automatically synchronizes data between regions, providing high availability and disaster recovery capabilities.\nStreams: DynamoDB Streams capture changes to your data in real-time and allow you to process these changes using AWS Lambda or other stream processing frameworks. This feature is useful for building event-driven architectures, implementing data synchronization, and triggering workflows based on database updates.\nAmazon Redshift: Amazon Redshift is a fully managed, petabyte-scale data warehouse service provided by Amazon Web Services (AWS). It is designed for high-performance analysis of large datasets using SQL queries. Some key features of Amazon Redshift include:\nKey Feature:\nColumnar Storage: Redshift stores data in a columnar format, which is highly optimized for analytics workloads. This allows for efficient data compression, reduced I/O, and faster query performance compared to traditional row-based storage systems.\nMassively Parallel Processing (MPP): Redshift distributes data and query processing across multiple nodes in a cluster, enabling parallel execution of queries. This architecture allows Redshift to scale horizontally as your data and query workload grow, providing consistent performance regardless of dataset size.\nScalability: Redshift supports clusters ranging from a few hundred gigabytes to multiple petabytes of data, allowing you to scale your data warehouse infrastructure based on your business needs. You can easily add or remove nodes to your Redshift cluster without downtime.\nHigh Performance: Redshift is optimized for fast query performance, with the ability to execute complex analytical queries against large datasets in seconds or minutes. It leverages advanced query optimization techniques, such as query parallelization, data distribution strategies, and automatic table statistics collection, to deliver optimal performance.\nIntegration with Data Lake: Redshift Spectrum enables you to query data stored in Amazon S3 directly from your Redshift cluster, without the need to load it into Redshift tables. This allows you to analyze data across your data warehouse and data lake seamlessly, providing a unified view of your data.\nAdvanced Analytics: Redshift supports advanced analytics capabilities, including window functions, user-defined functions (UDFs), and machine learning integration through Amazon SageMaker. This allows you to perform complex data analysis and predictive modeling directly within your Redshift environment.\nSecurity: Redshift provides robust security features, including encryption at rest and in transit, fine-grained access control using AWS Identity and Access Management (IAM) policies, and integration with AWS Key Management Service (KMS) for managing encryption keys. It is also compliant with various industry standards and regulations, such as HIPAA, GDPR, and PCI DSS.\nAutomated Backup and Maintenance: Redshift automatically takes incremental backups of your data and performs maintenance tasks such as software patches and node replacement, ensuring high availability and data durability without manual intervention.\nAmazon DocumentDB (with MongoDB compatibility): Amazon DocumentDB is a fully managed, MongoDB-compatible document database service provided by Amazon Web Services (AWS). It is designed to provide scalable, high-performance storage for JSON data, making it well-suited for applications that require flexible and dynamic data structures. Some key features of Amazon DocumentDB include:\nKey Feature:\nMongoDB Compatibility: Amazon DocumentDB is compatible with MongoDB 3.6, which means you can use existing MongoDB drivers, tools, and applications with DocumentDB without needing to modify your code. This compatibility makes it easy to migrate existing MongoDB workloads to DocumentDB with minimal effort.\nFully Managed Service: DocumentDB is a fully managed service, which means AWS handles tasks such as hardware provisioning, setup, configuration, monitoring, and backups. This allows developers to focus on building applications without worrying about database management tasks.\nScalability: DocumentDB is designed to scale effortlessly to accommodate growing workloads and data volumes. It supports horizontal scaling by automatically adding read replicas to distribute read traffic and improve performance. Additionally, DocumentDB can automatically scale storage capacity up to 64 TB per cluster, eliminating the need for manual capacity management.\nHigh Availability: DocumentDB provides built-in high availability with synchronous replication across multiple Availability Zones (AZs) within a region. This ensures that your data is resilient to AZ failures and remains available with minimal downtime or data loss.\nPerformance: DocumentDB offers fast and predictable performance for read-heavy workloads, with low-latency response times for query execution. It uses SSD-based storage and distributed processing to deliver high throughput and low-latency data access.\nDocument Model: DocumentDB supports a flexible document model based on JSON (BSON) documents, allowing you to store and query semi-structured data with nested fields and arrays. This makes it well-suited for applications with evolving schemas or dynamic data structures.\nSecurity: DocumentDB provides robust security features, including encryption at rest and in transit, fine-grained access control using AWS Identity and Access Management (IAM) policies, and integration with AWS Key Management Service (KMS) for managing encryption keys. It is also compliant with various industry standards and regulations, such as HIPAA, GDPR, and PCI DSS.\nManaged Backups and Point-in-Time Recovery: DocumentDB automatically takes continuous backups of your data and allows you to restore your database to any point in time within the backup retention period. This helps protect against data loss and provides a mechanism for disaster recovery.\nAmazon Neptune: Amazon Neptune is a fully managed graph database service provided by Amazon Web Services (AWS). It is optimized for storing and querying highly connected data, making it suitable for applications that require complex relationship modeling and traversal, such as social networks, recommendation engines, fraud detection, and knowledge graphs. Some key features of Amazon Neptune include:\nKey Feature:\nFully Managed Service: Neptune is a fully managed service, which means AWS handles infrastructure provisioning, setup, configuration, monitoring, backups, and maintenance tasks. This allows developers to focus on building applications without worrying about database management.\nGraph Database Engine: Neptune is built on a purpose-built graph database engine optimized for storing and querying graph-structured data. It supports property graph and RDF (Resource Description Framework) models, allowing you to represent data as nodes, edges, and properties.\nHighly Scalable: Neptune is designed to scale effortlessly to accommodate growing workloads and data volumes. It supports horizontal scaling by automatically adding read replicas and storage capacity to handle increasing query throughput and dataset sizes.\nHigh Availability: Neptune provides built-in high availability with synchronous replication across multiple Availability Zones (AZs) within a region. This ensures that your data is resilient to AZ failures and remains available with minimal downtime or data loss.\nPerformance: Neptune offers fast and predictable performance for graph queries, with low-latency response times for traversing relationships and analyzing graph data. It leverages optimized query execution plans and distributed processing to deliver high throughput and low-latency data access.\nFlexible Data Models: Neptune supports both property graph and RDF data models, providing flexibility in representing and querying graph-structured data. It allows you to define custom node and edge types, properties, and indexes to optimize query performance.\nIntegration with AWS Services: Neptune integrates seamlessly with other AWS services, such as Amazon S3, AWS Lambda, Amazon CloudWatch, and AWS Identity and Access Management (IAM). This allows you to build end-to-end graph-based applications using familiar AWS tools and services.\nSecurity: Neptune provides robust security features, including encryption at rest and in transit, fine-grained access control using AWS Identity and Access Management (IAM) policies, and integration with AWS Key Management Service (KMS) for managing encryption keys. It is also compliant with various industry standards and regulations, such as HIPAA, GDPR, and PCI DSS.\n"
},
{
	"uri": "//localhost:38235/2-hands-on/2.1-build-serverless-application/2.1.4.-create-lambda-function-and-deploy/",
	"title": "Create Lambda function and deploy",
	"tags": [],
	"description": "",
	"content": "Hands-on "
},
{
	"uri": "//localhost:38235/2-hands-on/2.1-build-serverless-application/2.1.3.-develop-lambda-function-with-java-language/",
	"title": "Develop Lambda Function with Java Language",
	"tags": [],
	"description": "",
	"content": "Project Structure ├── note-app-serverless\r│ ├── aws-resource\r│ │ ├── api-gw\r│ │ │ ├── api-config.json\r│ │ │ ├── api-output.json\r│ │ │ ├── api-resouce-out-put.json\r│ │ │ ├── instructions.md\r│ │ │ ├── Note Resources APP API-dev-oas30 (1).json\r│ │ │ └── Note Resources APP API-dev-oas30-postman (1).json\r│ │ ├── code-deploy\r│ │ │ └── pipeline.yaml\r│ │ └── lambda\r│ │ ├── LambdaAddDataFunction.yaml\r│ │ ├── LambdaDeleteNote.yaml\r│ │ └── LambdaFetchDataFunction.yaml\r│ └── aws-serverless-note-app\r│ ├── mvnw\r│ ├── mvnw.cmd\r│ ├── pom.xml\r│ ├── res.txt\r│ ├── src\r│ │ ├── main\r│ │ │ ├── java\r│ │ │ │ └── org\r│ │ │ │ └── example\r│ │ │ │ └── awsserverlessnoteapp\r│ │ │ │ ├── ApplicationMain.java\r│ │ │ │ ├── enviromentVariable.sh\r│ │ │ │ ├── LambdaAddDataFunction.java\r│ │ │ │ ├── LambdaDeleteDataFunction.java\r│ │ │ │ ├── LambdaFetchDataFunction.java\r│ │ │ │ └── LambdaUpdateDataFunction.java\r│ │ │ └── resources\r│ │ │ └── application.properties\r│ │ └── test\r│ │ └── java\r│ │ └── org\r│ │ └── example\r│ │ └── awsserverlessnoteapp\r│ │ └── AwsServerlessNoteAppApplicationTests.java\r│ └── target\r│ ├── aws-serverless-note-app-0.0.1-SNAPSHOT.jar\r│ ├── classes\r│ │ ├── application.properties\r│ │ └── org\r│ │ └── example\r│ │ └── awsserverlessnoteapp\r│ │ ├── ApplicationMain.class\r│ │ ├── LambdaAddDataFunction.class\r│ │ ├── LambdaDeleteDataFunction.class\r│ │ ├── LambdaFetchDataFunction.class\r│ │ └── LambdaUpdateDataFunction.class\r│ ├── generated-sources\r│ │ └── annotations\r│ ├── generated-test-sources\r│ │ └── test-annotations\r│ ├── maven-archiver\r│ │ └── pom.properties\r│ ├── maven-status\r│ │ └── maven-compiler-plugin\r│ │ ├── compile\r│ │ │ └── default-compile\r│ │ │ ├── createdFiles.lst\r│ │ │ └── inputFiles.lst\r│ │ └── testCompile\r│ │ └── default-testCompile\r│ │ ├── createdFiles.lst\r│ │ └── inputFiles.lst\r│ ├── original-aws-serverless-note-app-0.0.1-SNAPSHOT.jar\r│ └── test-classes\r└── README.md\rRefer My Gihub Repository and Let’s inspect the source code to understand how to create Lambda functions with Java.\nGit Url: https://github.com/daotq2000/aws-handson\nHere’s a breakdown of the relevant directories and files:\nnote-app-serverless:\nThis appears to be the root directory of your project.Inside this directory, there are two main subdirectories: aws-resource and aws-serverless-note-app. aws-resource:\nThis directory contains resources related to your AWS infrastructure setup. Inside aws-resource, there are subdirectories like api-gw, code-deploy, and lambda.api-gw likely contains configurations and definitions related to API Gateway.code-deploy appears to contain a pipeline definition file for AWS CodeDeploy.lambda likely contains configurations or definitions for your Lambda functions. aws-serverless-note-app:\nThis directory seems to contain the source code for your serverless note-taking app.Inside aws-serverless-note-app, there are directories like src and target, typical of a Maven project structure.The src directory contains your Java source code organized into main and test directories. The main directory contains the application’s main Java code, including classes like ApplicationMain.java, LambdaAddDataFunction.java, LambdaDeleteDataFunction.java, etc. The resources directory likely contains application configuration files, such as application.properties.\nHands-on Let start with pom.xml We needs 2 dependencies to develop core for lambda with Java Spring Boot Project Pom.xml\n\u0026lt;dependency\u0026gt;\r\u0026lt;groupId\u0026gt;com.amazonaws\u0026lt;/groupId\u0026gt;\r\u0026lt;artifactId\u0026gt;aws-java-sdk-dynamodb\u0026lt;/artifactId\u0026gt;\r\u0026lt;version\u0026gt;1.12.670\u0026lt;/version\u0026gt;\r\u0026lt;/dependency\u0026gt;\r\u0026lt;!-- https://mvnrepository.com/artifact/com.amazonaws/aws-lambda-java-core --\u0026gt;\r\u0026lt;dependency\u0026gt;\r\u0026lt;groupId\u0026gt;com.amazonaws\u0026lt;/groupId\u0026gt;\r\u0026lt;artifactId\u0026gt;aws-lambda-java-core\u0026lt;/artifactId\u0026gt;\r\u0026lt;version\u0026gt;1.2.3\u0026lt;/version\u0026gt;\r\u0026lt;/dependency\u0026gt;\rNext, we have to add plugin compile lambda function to Pom.xml file\n\u0026lt;plugins\u0026gt;\r\u0026lt;plugin\u0026gt;\r\u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt;\r\u0026lt;artifactId\u0026gt;maven-shade-plugin\u0026lt;/artifactId\u0026gt;\r\u0026lt;version\u0026gt;3.5.2\u0026lt;/version\u0026gt;\r\u0026lt;configuration\u0026gt;\r\u0026lt;createDependencyReducedPom\u0026gt;false\u0026lt;/createDependencyReducedPom\u0026gt;\r\u0026lt;/configuration\u0026gt;\r\u0026lt;executions\u0026gt;\r\u0026lt;execution\u0026gt;\r\u0026lt;phase\u0026gt;package\u0026lt;/phase\u0026gt;\r\u0026lt;goals\u0026gt;\r\u0026lt;goal\u0026gt;shade\u0026lt;/goal\u0026gt;\r\u0026lt;/goals\u0026gt;\r\u0026lt;/execution\u0026gt;\r\u0026lt;/executions\u0026gt;\r\u0026lt;/plugin\u0026gt;\r\u0026lt;/plugins\u0026gt;\rHere is full version file https://github.com/daotq2000/aws-handson/blob/main/note-app-serverless/aws-serverless-note-app/pom.xml\nNote: We need create 3 function: Add,Update Data and Fetch data and delete data. We must writing code under forder: note-app-serverless/aws-serverless-note-app/src/main/java/org/example/awsserverlessnoteapp\nFirst, let create Function LambdaAddDataFunction on file LambdaAddDataFunction.java Path: note-app-serverless/aws-serverless-note-app/src/main/java/org/example/awsserverlessnoteapp/LambdaAddDataFunction.java\npackage org.example.awsserverlessnoteapp;\rimport com.amazonaws.services.dynamodbv2.AmazonDynamoDBClient;\rimport com.amazonaws.services.dynamodbv2.model.AttributeValue;\rimport com.amazonaws.services.dynamodbv2.model.PutItemRequest;\rimport com.amazonaws.services.lambda.runtime.Context;\rimport com.amazonaws.services.lambda.runtime.RequestHandler;\rimport java.util.HashMap;\rimport java.util.Map;\rpublic class LambdaAddDataFunction implements RequestHandler\u0026lt;Map\u0026lt;String,String\u0026gt;,String\u0026gt; {\rprivate static final String TABLE_NAME = System.getenv(\u0026quot;AWS_DYNAMO_TABLE_NAME_VALUE\u0026quot;);\rprivate AmazonDynamoDBClient ddb;\rpublic LambdaAddDataFunction() {\rddb = (AmazonDynamoDBClient) AmazonDynamoDBClient.builder().withRegion(System.getenv(\u0026quot;AWS_REGION_VALUE\u0026quot;)).build();\r}\r@Override\rpublic String handleRequest(Map\u0026lt;String, String\u0026gt; itemValues, Context context) {\rMap\u0026lt;String, AttributeValue\u0026gt; item = new HashMap\u0026lt;\u0026gt;();\ritemValues.forEach((k, v) -\u0026gt; item.put(k, new AttributeValue().withS(v.toString())));\rPutItemRequest request = new PutItemRequest();\rrequest.setTableName(TABLE_NAME);\rrequest.setItem(item);\rddb.putItem(request);\rreturn item.toString();\r}\r}\rExplain: Because of Information of DynamoDB Table, Regions can change on Run Time, so we need expose it to environment by using System.getenv(“AWS_REGION_VALUE”) and System.getenv(“AWS_DYNAMO_TABLE_NAME_VALUE”) to get environment variables at runtime.\nprivate static final String TABLE_NAME = System.getenv(\u0026quot;AWS_DYNAMO_TABLE_NAME_VALUE\u0026quot;);\rpublic LambdaAddDataFunction() {\rddb = (AmazonDynamoDBClient) AmazonDynamoDBClient.builder().withRegion(System.getenv(\u0026quot;AWS_REGION_VALUE\u0026quot;)).build();\r}\rSecond, let create Function LambdaDeleteDataFunction on file LambdaDeleteDataFunction.java Path: note-app-serverless/aws-serverless-note-app/src/main/java/org/example/awsserverlessnoteapp/LambdaDeleteDataFunction.java\npackage org.example.awsserverlessnoteapp;\rimport com.amazonaws.services.dynamodbv2.AmazonDynamoDBClient;\rimport com.amazonaws.services.dynamodbv2.model.AttributeValue;\rimport com.amazonaws.services.dynamodbv2.model.DeleteItemRequest;\rimport com.amazonaws.services.lambda.runtime.Context;\rimport com.amazonaws.services.lambda.runtime.RequestHandler;\rimport java.util.HashMap;\rimport java.util.Map;\rpublic class LambdaDeleteDataFunction implements RequestHandler\u0026lt;Map\u0026lt;String,String\u0026gt;,String\u0026gt; {\rprivate static final String TABLE_NAME = System.getenv(\u0026quot;AWS_DYNAMO_TABLE_NAME_VALUE\u0026quot;);\rprivate AmazonDynamoDBClient ddb;\rpublic LambdaDeleteDataFunction() {\rddb = (AmazonDynamoDBClient) AmazonDynamoDBClient.builder().withRegion(System.getenv(\u0026quot;AWS_REGION_VALUE\u0026quot;)).build();\r}\r@Override\rpublic String handleRequest(Map\u0026lt;String, String\u0026gt; itemValues, Context context) {\rMap\u0026lt;String, AttributeValue\u0026gt; key = new HashMap\u0026lt;\u0026gt;();\rkey.put(\u0026quot;id\u0026quot;, new AttributeValue().withS(itemValues.get(\u0026quot;id\u0026quot;)));\rDeleteItemRequest request = new DeleteItemRequest()\r.withTableName(TABLE_NAME)\r.withKey(key);\rddb.deleteItem(request);\rreturn \u0026quot;Delete successfully\u0026quot;;\r}\r}\rSecond, let create Function LambdaFetchDataFunction on file LambdaFetchDataFunction.java Path: note-app-serverless/aws-serverless-note-app/src/main/java/org/example/awsserverlessnoteapp/LambdaFetchDataFunction.java\npackage org.example.awsserverlessnoteapp;\rimport com.amazonaws.services.dynamodbv2.AmazonDynamoDBClient;\rimport com.amazonaws.services.dynamodbv2.model.*;\rimport com.amazonaws.services.lambda.runtime.Context;\rimport com.amazonaws.services.lambda.runtime.RequestHandler;\rimport java.util.ArrayList;\rimport java.util.HashMap;\rimport java.util.List;\rimport java.util.Map;\rpublic class LambdaFetchDataFunction implements RequestHandler\u0026lt;Map\u0026lt;String,String\u0026gt;, List\u0026lt;Map\u0026lt;String, String\u0026gt;\u0026gt;\u0026gt; {\rprivate static final String TABLE_NAME = System.getenv(\u0026quot;AWS_DYNAMO_TABLE_NAME_VALUE\u0026quot;);\rprivate AmazonDynamoDBClient ddb;\rpublic LambdaFetchDataFunction() {\rddb = (AmazonDynamoDBClient) AmazonDynamoDBClient.builder().withRegion(System.getenv(\u0026quot;AWS_REGION_VALUE\u0026quot;)).build();\r}\r@Override\rpublic List\u0026lt;Map\u0026lt;String,String\u0026gt;\u0026gt; handleRequest(Map\u0026lt;String, String\u0026gt; itemValues, Context context) {\rScanRequest request = new ScanRequest()\r.withTableName(TABLE_NAME);\rScanResult response = ddb.scan(request);\rList\u0026lt;Map\u0026lt;String, String\u0026gt;\u0026gt; result = new ArrayList\u0026lt;\u0026gt;();\rfor (Map\u0026lt;String, AttributeValue\u0026gt; item : response.getItems()) {\rMap\u0026lt;String, String\u0026gt; newItem = new HashMap\u0026lt;\u0026gt;();\rfor (Map.Entry\u0026lt;String, AttributeValue\u0026gt; entry : item.entrySet()) {\rnewItem.put(entry.getKey(), entry.getValue().getS());\r}\rresult.add(newItem);\r}\rreturn result;\r}\r}\rAfter done, we can build project with Maven command\ncd note-app-serverless/aws-serverless-note-app \u0026amp;\u0026amp; mvn clean package\rIf you don\u0026rsquo;t have Maven,\nyou can download at Home Page https://maven.apache.org/install.html Install maven with ubuntu: sudo apt install maven If you received response below, you are successfully compile these function\nINFO] Replacing original artifact with shaded artifact.\r[INFO] Replacing /home/ubuntu-server/Documents/git/github/aws-handson/note-app-serverless/aws-serverless-note-app/target/aws-serverless-note-app-0.0.1-SNAPSHOT.jar with /home/ubuntu-server/Documents/git/github/aws-handson/note-app-serverless/aws-serverless-note-app/target/aws-serverless-note-app-0.0.1-SNAPSHOT-shaded.jar\r[INFO] ------------------------------------------------------------------------\r[INFO] BUILD SUCCESS\r[INFO] ------------------------------------------------------------------------\r[INFO] Total time: 4.656 s\r[INFO] Finished at: 2024-03-05T00:46:40+07:00\r[INFO] ------------------------------------------------------------------------ "
},
{
	"uri": "//localhost:38235/1-introduce-aws/1.4-networkingservices/",
	"title": "Networking Service",
	"tags": [],
	"description": "",
	"content": "Introduction Networking Service Amazon Web Services (AWS) offers a comprehensive suite of networking services designed to provide scalable, secure, and reliable connectivity for cloud-based applications and resources. These networking services enable businesses to build, manage, and optimize their network infrastructure with ease. Here\u0026rsquo;s an introduction to some key AWS networking services:\nAWS offers several database services, each catering to different use cases and workload requirements Contents Amazon Virtual Private Cloud (VPC): AAmazon Virtual Private Cloud (VPC) is its ability to provide users with complete control over their virtual networking environment in the AWS Cloud\nKey feature:\nIsolation: Amazon VPC enables users to create isolated sections of the AWS Cloud, known as virtual private clouds. This isolation provides enhanced security by allowing users to define their own network topology, configure their own IP address range, and implement network access control policies.\nCustomization: Users have the flexibility to customize their VPCs by creating subnets, defining route tables, configuring network gateways (such as Internet Gateways, Virtual Private Gateways, and NAT Gateways), and setting up security groups and network access control lists (ACLs) to control traffic flow.\nScalability: Amazon VPC is highly scalable, allowing users to easily scale their network infrastructure up or down based on changing requirements. Users can dynamically add or remove resources within their VPC, such as instances, storage, and networking components, without disrupting their operations.\nIntegration: Amazon VPC seamlessly integrates with other AWS services, enabling users to build complex and distributed architectures for their applications. Users can deploy AWS resources such as Amazon EC2 instances, Amazon RDS databases, and Amazon S3 storage buckets within their VPCs, providing them with secure and private access to these resources.\nConnectivity Options: Amazon VPC offers multiple connectivity options for connecting to other networks and resources. Users can establish private connectivity between their VPC and on-premises data centers using AWS Direct Connect or VPN connections, enabling hybrid cloud deployments. Additionally, users can connect multiple VPCs together using VPC peering or AWS Transit Gateway, allowing them to build complex and interconnected network architectures.\nAmazon Route 53: Amazon Route 53, AWS\u0026rsquo;s scalable Domain Name System (DNS) web service, offers several key features that make it a critical component for managing domain names and routing internet traffic efficiently.\nKey Feature:\nHigh Availability and Reliability: Amazon Route 53 is designed to provide high availability and reliability for DNS queries. It operates on a highly distributed and redundant global network of DNS servers, which helps ensure low latency and minimal downtime for DNS resolution.\nDomain Registration: Route 53 allows users to register and manage domain names directly from the AWS Management Console. Users can easily register new domains or transfer existing ones, and manage domain registration settings such as contact information, privacy protection, and domain auto-renewal.\nDNS Routing Policies: Route 53 offers a variety of DNS routing policies that allow users to control how DNS queries are routed and distributed. These policies include Simple Routing, Weighted Routing, Latency-based Routing, Failover Routing, Geolocation Routing, and Multi-Value Answer Routing, enabling users to implement sophisticated traffic routing strategies based on factors such as geographic location, latency, health checks, and more.\nHealth Checks and Failover: Route 53 provides integrated health checking capabilities that allow users to monitor the health and availability of their resources, such as web servers or load balancers. Users can configure health checks to periodically check the status of their resources and automatically route traffic away from unhealthy or unavailable endpoints to healthy ones using DNS Failover.\nIntegration with AWS Services: Route 53 seamlessly integrates with other AWS services, making it easy to manage DNS records for AWS resources. Users can automatically create DNS records for resources such as Amazon EC2 instances, Elastic Load Balancers, Amazon S3 buckets, and CloudFront distributions, simplifying the process of configuring DNS for cloud-based applications and services.\nTraffic Flow Visualization: Route 53 provides a visual Traffic Flow feature that allows users to create and manage complex traffic routing configurations using a graphical interface. Users can easily visualize how traffic flows through their infrastructure and make changes to routing policies in real-time, helping them optimize performance, reliability, and cost-effectiveness.\nAWS Direct Connect One of the key features of AWS Direct Connect is its ability to establish private, dedicated network connections between an organization\u0026rsquo;s on-premises data center, office, or colocation environment and AWS\u0026rsquo;s infrastructure.\nKey Feature:\nPrivate Connectivity: AWS Direct Connect provides private connectivity to AWS\u0026rsquo;s cloud services, bypassing the public internet. This ensures a consistent and predictable network experience with lower latency, higher throughput, and improved security compared to internet-based connections.\nDedicated Connections: Users can establish dedicated 1 Gbps or 10 Gbps connections between their network and AWS Direct Connect locations, providing reliable and high-bandwidth connectivity for their workloads and applications.\nFlexible Bandwidth Options: AWS Direct Connect offers flexible bandwidth options to meet the varying needs of different organizations. Users can choose from multiple bandwidth levels, ranging from 50 Mbps to 100 Gbps, and can easily scale up or down their bandwidth capacity as needed.\nReduced Network Costs: By leveraging AWS Direct Connect, organizations can reduce their network costs by avoiding data transfer fees associated with internet-based connections and by optimizing their network infrastructure for efficient data transfer to and from AWS\u0026rsquo;s cloud services.\nImproved Security: AWS Direct Connect helps improve the security of data transfer between an organization\u0026rsquo;s network and AWS\u0026rsquo;s infrastructure by providing dedicated and private network connections. Users can implement additional security measures, such as encryption and access control policies, to further enhance the security of their data.\nHybrid Cloud Connectivity: AWS Direct Connect enables hybrid cloud connectivity by allowing organizations to seamlessly integrate their on-premises infrastructure with AWS\u0026rsquo;s cloud services. This enables organizations to extend their existing data center investments, migrate workloads to the cloud, and build hybrid cloud architectures that leverage the benefits of both on-premises and cloud environments.\nGlobal Reach: AWS Direct Connect is available in multiple regions around the world, with Direct Connect locations located in major cities and data centers. This global reach enables organizations to establish private connectivity to AWS\u0026rsquo;s infrastructure from virtually anywhere, making it suitable for multinational deployments and global businesses.\nAmazon CloudFront Content Delivery Network (CDN) Service: Amazon CloudFront is a CDN service that caches content at edge locations distributed around the world. By caching content closer to end users, CloudFront reduces latency and improves the performance of web applications, APIs, videos, and other content. Key Feature:\nGlobal Network of Edge Locations: CloudFront operates on a global network of edge locations, which are strategically located in major cities and regions worldwide. These edge locations serve as caching endpoints where content is stored and delivered to users, enabling CloudFront to provide low-latency access to content regardless of the user\u0026rsquo;s geographic location.\nHigh Availability and Reliability: CloudFront is designed to provide high availability and reliability for content delivery. It automatically routes user requests to the nearest available edge location with the lowest latency, ensuring fast and reliable delivery of content to end users.\nScalability: CloudFront is highly scalable and can handle traffic spikes and fluctuations in demand without any additional configuration or provisioning. It dynamically scales resources based on demand, allowing users to deliver content to millions of concurrent users with ease.\nSecurity Features: CloudFront offers a range of security features to help protect content and applications from common security threats. These features include support for HTTPS encryption, SSL/TLS certificate management, access control using signed URLs and cookies, IP whitelisting and blacklisting, and integration with AWS Web Application Firewall (WAF) for additional security.\nIntegration with AWS Services: CloudFront seamlessly integrates with other AWS services, making it easy to deliver content stored in Amazon S3 buckets, Amazon EC2 instances, AWS Lambda functions, and other origin servers. Users can configure CloudFront distributions to cache and deliver content from multiple origins, enabling them to build highly dynamic and scalable applications.\nReal-Time Metrics and Monitoring: CloudFront provides real-time metrics and monitoring capabilities that allow users to monitor the performance and usage of their distributions. Users can track key metrics such as request counts, data transfer, cache hit ratios, and error rates using CloudFront\u0026rsquo;s built-in monitoring tools or by integrating with AWS CloudWatch for advanced monitoring and alerting.\n"
},
{
	"uri": "//localhost:38235/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:38235/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]